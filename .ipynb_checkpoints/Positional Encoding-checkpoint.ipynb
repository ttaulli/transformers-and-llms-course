{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "505191df-86c4-4174-a7f7-e15c01f11e3e",
   "metadata": {},
   "source": [
    "# Positional Encoding in Action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80aa1b3b-26ca-462d-8396-e4643a1ba422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3934964f-5423-4c1a-ad3a-be99d082cf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164ac8d-2b5a-4978-9509-e64c32e4b667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(max_len=50, d_model=64):\n",
    "    PE = torch.zeros(max_len, d_model)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "    PE[:, 0::2] = torch.sin(position * div_term)  # Sine on even dims\n",
    "    PE[:, 1::2] = torch.cos(position * div_term)  # Cosine on odd dims\n",
    "    \n",
    "    return PE\n",
    "\n",
    "PE = positional_encoding()\n",
    "PE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2adb71-2879-4556-afe7-d384ceb2a966",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Positional Encoding Heatmap (Positions 0–50)\")\n",
    "sns.heatmap(PE, cmap=\"viridis\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e44cd15-927b-4aba-8832-6d5608f9cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for pos in [0, 5, 15, 30]:\n",
    "    plt.plot(PE[pos].numpy(), label=f\"Position {pos}\")\n",
    "\n",
    "plt.title(\"Encoding Patterns for Different Token Positions\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Encoding Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731a3948-6cba-4d1b-a69a-65ef351a555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "bert_positional = model.embeddings.position_embeddings.weight.data[:50]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(bert_positional, cmap=\"coolwarm\")\n",
    "plt.title(\"BERT Learned Positional Embeddings (Positions 0–50)\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bb609-b3be-473b-9605-3d4157ce781e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
